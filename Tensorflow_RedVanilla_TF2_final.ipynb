{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA0FvFe966r9"
      },
      "source": [
        "# Tarea 1_ Red neuronal Perceptrón multicapa con Tensorflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Se importan librerias necesarias para el funcionamiento de nuestro MLP**"
      ],
      "metadata": {
        "id": "pB8ERgTPBL8R"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "babYmP-H61wB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfe90274-0acf-4efd-cd2d-1a57c9d1be25"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP5zZjkD7sWw"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Se cargan desde los datasets de keras y tensorflow el conjunto de datos fashion_mnist**"
      ],
      "metadata": {
        "id": "sew2YIviBOjW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl5FegN27wzO"
      },
      "source": [
        "from tensorflow.keras.datasets.fashion_mnist import load_data\n",
        "fashion_mnist = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq-Bj_Kar14P"
      },
      "source": [
        "## Separando el conjunto de datos en Entrenamiento y prueba "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Se separa en entrenamiento y test el conjunto de pruebas**"
      ],
      "metadata": {
        "id": "diBSdeYMBjFk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjokko6Mr14Q"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test)=fashion_mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aquí se puede observar que el conjunto de entrenamiento tiene 60,000 imágenes en una dimensión de 28x28**"
      ],
      "metadata": {
        "id": "DsKnusO5Bm61"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_1FAsMYr14W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb57e61-3662-4a50-d2cc-68972d086a22"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**En la celda 72 y 73 se muestra la visualización de un elemento correspondiente al conjunto de entrenamiento**"
      ],
      "metadata": {
        "id": "eM1L3RVxB3zD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwUO3WIUr14b"
      },
      "source": [
        "imagendemo=x_train[100,:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkOVIgvnr14g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "3fc88809-0caa-405f-c527-59082728922b"
      },
      "source": [
        "plt.imshow(imagendemo,cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc842324190>"
            ]
          },
          "metadata": {},
          "execution_count": 95
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hacemos un reshape para que el tensor tome la dimensión de 60,000 x 784 (este 784 sale porque multiplicamos la dimensión original que es 28x28**"
      ],
      "metadata": {
        "id": "fUI9zJ0mB_WT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txiz-glb4Uq2",
        "outputId": "a6861d01-3088-429d-88d0-74cec18df76f"
      },
      "source": [
        "x_train=x_train.reshape(-1,28*28).astype('float32')\n",
        "x_test=x_test.reshape(-1,28*28).astype('float32')\n",
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g558R0wo4jbR"
      },
      "source": [
        "**Hacemos la codificación one hot de las etiquetas de entrenamiento y validación. Recordando que la códificación one hot nos da un vector binario del tamaño del total de clases donde un 0 es \"no corresponde a la clase\" y un 1 es \"corresponde a la clase\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joI4yzjr4irw",
        "outputId": "7f6c97b3-7b51-4d18-cb32-4ec001b1058a"
      },
      "source": [
        "# onehot encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "y_train = y_train.reshape(len(y_train), 1)\n",
        "y_train_onehot = onehot_encoder.fit_transform(y_train)\n",
        "y_train_onehot.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjIhXcc5DlIw",
        "outputId": "7580434c-2c1e-4753-ac15-dfe1d37c9b8b"
      },
      "source": [
        "y_test = y_test.reshape(len(y_test), 1)\n",
        "y_test_onehot = onehot_encoder.fit_transform(y_test)\n",
        "y_test_onehot.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test_onehot[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOI0XEqZZGyy",
        "outputId": "df05dda3-66e5-4784-b049-d6d19fd265af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwRo-mx_r14l"
      },
      "source": [
        "Las etiquetas numéricas pueden ser transformadas al nombre de la clase correspondiente usando el siguiente diccionario "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypNFROm8r14m"
      },
      "source": [
        "label_dict = {\n",
        " 0: \"T-shirt/top\",\n",
        " 1: \"Trouser\",\n",
        " 2: \"Pullover\",\n",
        " 3: \"Dress\",\n",
        " 4: \"Coat\",\n",
        " 5: \"Sandal\",\n",
        " 6: \"Shirt\",\n",
        " 7: \"Sneaker\",\n",
        " 8: \"Bag\",\n",
        " 9: \"Ankle boot\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edjTF1COr14q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0e248ea0-6910-463d-e491-9c0e0f0fed17"
      },
      "source": [
        "label_dict[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'T-shirt/top'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiNbn0oX5F4u"
      },
      "source": [
        "## Arquitectura\n",
        "**Se genera el modelo que corresponde a nuestra arquitectura. Para este modelo se implementan 3 capas donde las variables n_nodes_hlx (x va de 1 a 3) corresponden a las capas. Finalmente se agrega una capa de salida que contiene únicamente las 10 clases que clasificará nuestra red.**\n",
        "\n",
        "**En las línas 8-9, 11-12, 14-15 y 17-18 multiplico por 0.1 esto corresponde a un método de regularización para que los pesos no inicien en números muy grandes, pues como se ve en los resultados de entrenamiento, ayuda a obtener mejores resultados de entrenamiento.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y43A7YP4Nlg"
      },
      "source": [
        "class DNN_model(object):\n",
        "  def __init__(self,\n",
        "               n_nodes_hl1=340,\n",
        "               n_nodes_hl2=200,\n",
        "               n_nodes_hl3=100,\n",
        "               n_classes=10):\n",
        "    \n",
        "    self.h1LW = tf.Variable(np.random.rand(784, n_nodes_hl1)*0.1,name=\"hl1weigths\",dtype=\"float32\")\n",
        "    self.h1LB = tf.Variable(np.random.rand(n_nodes_hl1)*0.1,name=\"hl1bias\",dtype=\"float32\")\n",
        "\n",
        "    self.h2LW = tf.Variable(np.random.rand(n_nodes_hl1, n_nodes_hl2)*0.1,name=\"hl2weigths\",dtype=\"float32\")\n",
        "    self.h2LB = tf.Variable(np.random.rand(n_nodes_hl2)*0.1,name=\"hl2bias\",dtype=\"float32\")\n",
        "\n",
        "    self.h3LW = tf.Variable(np.random.rand(n_nodes_hl2, n_nodes_hl3)*0.1,name=\"hl3weigths\",dtype=\"float32\")\n",
        "    self.h3LB = tf.Variable(np.random.rand(n_nodes_hl3)*0.1,name=\"hl3bias\",dtype=\"float32\")\n",
        "\n",
        "    self.outW = tf.Variable(np.random.rand(n_nodes_hl3, n_classes)*0.1,name=\"outweigths\",dtype=\"float32\")\n",
        "    self.outB = tf.Variable(np.random.rand(n_classes)*0.1,name=\"outbias\",dtype=\"float32\")\n",
        "\n",
        "    self.trainable_variables =[self.h1LW,self.h1LB,\n",
        "                               self.h2LW,self.h2LB,\n",
        "                               self.h3LW,self.h3LB,\n",
        "\n",
        "                               self.outW,self.outB]\n",
        "\n",
        "  def __call__(self,x): \n",
        "      # Declarando la arquitectura\n",
        "\n",
        "      l1 = tf.add(tf.matmul(x,self.h1LW), self.h1LB)\n",
        "      l1 = tf.nn.relu(l1)\n",
        "\n",
        "      l2 = tf.add(tf.matmul(l1,self.h2LW), self.h2LB)\n",
        "      l2 = tf.nn.relu(l2)\n",
        "\n",
        "      l3 = tf.add(tf.matmul(l2,self.h3LW), self.h3LB)\n",
        "      l3 = tf.nn.relu(l3)\n",
        "\n",
        "      output = tf.matmul(l3,self.outW) + self.outB\n",
        "      return output\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDPEM3b54Ncd"
      },
      "source": [
        "DNN = DNN_model()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iChZs21P5jmw"
      },
      "source": [
        "Seleccionar un optimizador\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tal como se observa en el estado del arte actual, el optimizador Adam sigue siendo uno de los que mejores resultados dan, es por ello que opté por incorporarlo. Como datoe extra, seleccioné un LR de 0.001 para que no diera brincos grandes, es decir, que diera explotación en lugar de exploración**"
      ],
      "metadata": {
        "id": "-1PK3v4mD3Rq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZaC_7i_4NY-"
      },
      "source": [
        "optimizador = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6FlakSx6fc1"
      },
      "source": [
        "## Definir las métricas a utilizar"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**En la celda 83 se definen las métricas que utilizaré al entrenamiento, es decir, aquellas que arrojará como resultado. Cabe aclarar que todas estas provienen de CategoricalAccuracy porque tenemos varias clases, por lo tanto es la ideal**."
      ],
      "metadata": {
        "id": "Php21dWdELCb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3FtV-7J4NIt"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvVptRPA6pWd"
      },
      "source": [
        "## Cálculo de gradiente y ajuste\n",
        "T R A I N"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wdq6I0y6k-x"
      },
      "source": [
        "@tf.function # cabezal para hacerlo compilado\n",
        "def train_step(model,tdata, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(tdata)\n",
        "    #calculo de una función de error \n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels, predictions))\n",
        "   \n",
        "  #cálculo de los gradientes\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  #ordenamiento de los gradientes \n",
        "  capped_grads_and_vars = [(grad,model.trainable_variables[index]) for index, grad in enumerate(gradients)]\n",
        "  #aplicación de la regla de ajuste\n",
        "  optimizador.apply_gradients(capped_grads_and_vars)\n",
        "\n",
        "  train_loss(loss) #error por época\n",
        "  train_accuracy(labels, predictions) #accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzjQolNe6yz2"
      },
      "source": [
        "T E S T "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5TVXhMn6wvC"
      },
      "source": [
        "@tf.function\n",
        "def test_step(model,tdata, labels):\n",
        "  predictions = model(tdata)\n",
        "  t_loss =  tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels, predictions))\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Agi2xS07Cme"
      },
      "source": [
        "Función de entrenamiento y prueba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BptTnqkI6-9w"
      },
      "source": [
        "def fitting(model, train_x, train_y, test_x, test_y, EPOCHS, N_batch, batch_size):\n",
        "  for epoch in range(EPOCHS):\n",
        "    i=0\n",
        "    while i+batch_size < len(train_x) or i+batch_size<batch_size*N_batch:\n",
        "      start = i\n",
        "      end = i+batch_size\n",
        "      batch_x = train_x[start:end]\n",
        "      batch_y = train_y[start:end]\n",
        "      train_step(model,batch_x,batch_y)\n",
        "      i+=batch_size\n",
        "\n",
        "    test_step(model,test_x,test_y)\n",
        "      \n",
        "    template = 'Epoch {}, Perdida: {}, Exactitud: {}, Perdida de prueba: {}, Exactitud de prueba: {}'\n",
        "    print(template.format(epoch+1,\n",
        "                         train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result()*100))\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Se seleccionó un total de 250 epocas para ver si mejoraba el entrenamiento,\n",
        "sin embargo, como se observa en el entrenamiento, a partir de la época 120, empieza a oscilar únicamente entre 88% y 87% de exactitud de prueba, mientras que la exactitud sin evaluación alcanza el 95%**"
      ],
      "metadata": {
        "id": "a9XA3VmHEtpi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnH9f7xr6jyR",
        "outputId": "40447135-c714-4e7a-bf4a-be13d21d2f23"
      },
      "source": [
        "num_epochs = 250\n",
        "batch = 228\n",
        "fitting(DNN, x_train, y_train_onehot, x_test, y_test_onehot, num_epochs, int(60000/batch), batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Perdida: 7963.54736328125, Exactitud: 20.045360565185547, Perdida de prueba: 15.568765640258789, Exactitud de prueba: 48.33000183105469\n",
            "Epoch 2, Perdida: 7.862791061401367, Exactitud: 61.88212585449219, Perdida de prueba: 5.292470932006836, Exactitud de prueba: 66.50999450683594\n",
            "Epoch 3, Perdida: 4.027539253234863, Exactitud: 70.782470703125, Perdida de prueba: 2.673490524291992, Exactitud de prueba: 73.69999694824219\n",
            "Epoch 4, Perdida: 2.4750216007232666, Exactitud: 74.48468780517578, Perdida de prueba: 1.812242031097412, Exactitud de prueba: 77.61000061035156\n",
            "Epoch 5, Perdida: 1.6572870016098022, Exactitud: 77.3147201538086, Perdida de prueba: 1.8772214651107788, Exactitud de prueba: 76.02999877929688\n",
            "Epoch 6, Perdida: 1.2970364093780518, Exactitud: 78.86398315429688, Perdida de prueba: 0.9016775488853455, Exactitud de prueba: 81.3499984741211\n",
            "Epoch 7, Perdida: 0.8804863095283508, Exactitud: 80.30985260009766, Perdida de prueba: 0.8933501243591309, Exactitud de prueba: 78.20999908447266\n",
            "Epoch 8, Perdida: 0.6949491500854492, Exactitud: 81.31378936767578, Perdida de prueba: 0.6809391379356384, Exactitud de prueba: 80.72000122070312\n",
            "Epoch 9, Perdida: 0.5849499702453613, Exactitud: 82.53619384765625, Perdida de prueba: 0.6476319432258606, Exactitud de prueba: 81.63999938964844\n",
            "Epoch 10, Perdida: 0.5277974009513855, Exactitud: 83.25995635986328, Perdida de prueba: 0.5628030300140381, Exactitud de prueba: 82.20999908447266\n",
            "Epoch 11, Perdida: 0.46546265482902527, Exactitud: 84.58074951171875, Perdida de prueba: 0.5067139267921448, Exactitud de prueba: 83.71000671386719\n",
            "Epoch 12, Perdida: 0.44057318568229675, Exactitud: 85.30284881591797, Perdida de prueba: 0.4764285683631897, Exactitud de prueba: 84.44000244140625\n",
            "Epoch 13, Perdida: 0.4152192175388336, Exactitud: 85.93989562988281, Perdida de prueba: 0.49093276262283325, Exactitud de prueba: 84.04000091552734\n",
            "Epoch 14, Perdida: 0.3946714401245117, Exactitud: 86.5836181640625, Perdida de prueba: 0.5036215782165527, Exactitud de prueba: 83.95000457763672\n",
            "Epoch 15, Perdida: 0.3776600956916809, Exactitud: 87.0705795288086, Perdida de prueba: 0.5000413060188293, Exactitud de prueba: 84.16000366210938\n",
            "Epoch 16, Perdida: 0.3629724383354187, Exactitud: 87.45413970947266, Perdida de prueba: 0.48372167348861694, Exactitud de prueba: 84.77999877929688\n",
            "Epoch 17, Perdida: 0.3496163487434387, Exactitud: 87.80935668945312, Perdida de prueba: 0.4750176668167114, Exactitud de prueba: 84.90999603271484\n",
            "Epoch 18, Perdida: 0.3389611542224884, Exactitud: 88.17456817626953, Perdida de prueba: 0.47713232040405273, Exactitud de prueba: 85.16999816894531\n",
            "Epoch 19, Perdida: 0.32852673530578613, Exactitud: 88.53145599365234, Perdida de prueba: 0.49112051725387573, Exactitud de prueba: 85.0999984741211\n",
            "Epoch 20, Perdida: 0.3196558952331543, Exactitud: 88.80828857421875, Perdida de prueba: 0.495014488697052, Exactitud de prueba: 85.18000030517578\n",
            "Epoch 21, Perdida: 0.3070947527885437, Exactitud: 89.23187255859375, Perdida de prueba: 0.48306551575660706, Exactitud de prueba: 85.38999938964844\n",
            "Epoch 22, Perdida: 0.295596718788147, Exactitud: 89.62877655029297, Perdida de prueba: 0.4806649386882782, Exactitud de prueba: 85.45999908447266\n",
            "Epoch 23, Perdida: 0.2861078977584839, Exactitud: 89.95063781738281, Perdida de prueba: 0.4676496982574463, Exactitud de prueba: 85.54000091552734\n",
            "Epoch 24, Perdida: 0.276549756526947, Exactitud: 90.2591552734375, Perdida de prueba: 0.4485398530960083, Exactitud de prueba: 85.87999725341797\n",
            "Epoch 25, Perdida: 0.26577967405319214, Exactitud: 90.59768676757812, Perdida de prueba: 0.4186842739582062, Exactitud de prueba: 86.58000183105469\n",
            "Epoch 26, Perdida: 0.25794562697410583, Exactitud: 90.87619018554688, Perdida de prueba: 0.4052627980709076, Exactitud de prueba: 86.76000213623047\n",
            "Epoch 27, Perdida: 0.25092577934265137, Exactitud: 91.07464599609375, Perdida de prueba: 0.40626591444015503, Exactitud de prueba: 86.98999786376953\n",
            "Epoch 28, Perdida: 0.24489299952983856, Exactitud: 91.29644775390625, Perdida de prueba: 0.3947170376777649, Exactitud de prueba: 87.15999603271484\n",
            "Epoch 29, Perdida: 0.2384045273065567, Exactitud: 91.42819213867188, Perdida de prueba: 0.4034709632396698, Exactitud de prueba: 86.98999786376953\n",
            "Epoch 30, Perdida: 0.23355115950107574, Exactitud: 91.58328247070312, Perdida de prueba: 0.3930104076862335, Exactitud de prueba: 87.45999908447266\n",
            "Epoch 31, Perdida: 0.2273053228855133, Exactitud: 91.82342529296875, Perdida de prueba: 0.4062642455101013, Exactitud de prueba: 86.80999755859375\n",
            "Epoch 32, Perdida: 0.22070570290088654, Exactitud: 91.99185943603516, Perdida de prueba: 0.40070557594299316, Exactitud de prueba: 87.08000183105469\n",
            "Epoch 33, Perdida: 0.22102861106395721, Exactitud: 92.02188110351562, Perdida de prueba: 0.40355291962623596, Exactitud de prueba: 87.19999694824219\n",
            "Epoch 34, Perdida: 0.2248559445142746, Exactitud: 91.80007934570312, Perdida de prueba: 0.4979509711265564, Exactitud de prueba: 84.12000274658203\n",
            "Epoch 35, Perdida: 0.22991511225700378, Exactitud: 91.50489807128906, Perdida de prueba: 0.39769208431243896, Exactitud de prueba: 87.37999725341797\n",
            "Epoch 36, Perdida: 0.22906357049942017, Exactitud: 91.52491760253906, Perdida de prueba: 0.4002084732055664, Exactitud de prueba: 87.2699966430664\n",
            "Epoch 37, Perdida: 0.23097464442253113, Exactitud: 91.43318939208984, Perdida de prueba: 0.4462157189846039, Exactitud de prueba: 86.36000061035156\n",
            "Epoch 38, Perdida: 0.23641717433929443, Exactitud: 91.27976989746094, Perdida de prueba: 0.5006915926933289, Exactitud de prueba: 84.91999816894531\n",
            "Epoch 39, Perdida: 0.24161814153194427, Exactitud: 90.9645767211914, Perdida de prueba: 0.4830384850502014, Exactitud de prueba: 86.25\n",
            "Epoch 40, Perdida: 0.24636510014533997, Exactitud: 90.79114532470703, Perdida de prueba: 0.4234580099582672, Exactitud de prueba: 86.63999938964844\n",
            "Epoch 41, Perdida: 0.24628427624702454, Exactitud: 90.81449127197266, Perdida de prueba: 0.4396478533744812, Exactitud de prueba: 86.04000091552734\n",
            "Epoch 42, Perdida: 0.24717198312282562, Exactitud: 90.88787078857422, Perdida de prueba: 0.4493913948535919, Exactitud de prueba: 86.3699951171875\n",
            "Epoch 43, Perdida: 0.2523030936717987, Exactitud: 90.59268951416016, Perdida de prueba: 0.4690135717391968, Exactitud de prueba: 86.33999633789062\n",
            "Epoch 44, Perdida: 0.25078102946281433, Exactitud: 90.72109985351562, Perdida de prueba: 0.46048760414123535, Exactitud de prueba: 86.0\n",
            "Epoch 45, Perdida: 0.24255046248435974, Exactitud: 91.00460052490234, Perdida de prueba: 0.4329853653907776, Exactitud de prueba: 86.68000030517578\n",
            "Epoch 46, Perdida: 0.23334772884845734, Exactitud: 91.20806121826172, Perdida de prueba: 0.474346399307251, Exactitud de prueba: 86.19999694824219\n",
            "Epoch 47, Perdida: 0.23784470558166504, Exactitud: 91.12300872802734, Perdida de prueba: 0.4908981919288635, Exactitud de prueba: 85.33999633789062\n",
            "Epoch 48, Perdida: 0.22629480063915253, Exactitud: 91.57827758789062, Perdida de prueba: 0.4991728663444519, Exactitud de prueba: 85.62999725341797\n",
            "Epoch 49, Perdida: 0.23040416836738586, Exactitud: 91.47988891601562, Perdida de prueba: 0.4551927149295807, Exactitud de prueba: 86.61000061035156\n",
            "Epoch 50, Perdida: 0.22361302375793457, Exactitud: 91.59162139892578, Perdida de prueba: 0.45186591148376465, Exactitud de prueba: 86.15999603271484\n",
            "Epoch 51, Perdida: 0.22508133947849274, Exactitud: 91.56159973144531, Perdida de prueba: 0.4640056788921356, Exactitud de prueba: 85.93999481201172\n",
            "Epoch 52, Perdida: 0.21642088890075684, Exactitud: 91.8851318359375, Perdida de prueba: 0.43178868293762207, Exactitud de prueba: 86.20999908447266\n",
            "Epoch 53, Perdida: 0.20990590751171112, Exactitud: 92.29705047607422, Perdida de prueba: 0.49783065915107727, Exactitud de prueba: 85.47999572753906\n",
            "Epoch 54, Perdida: 0.20721615850925446, Exactitud: 92.22200012207031, Perdida de prueba: 0.4386272430419922, Exactitud de prueba: 87.05999755859375\n",
            "Epoch 55, Perdida: 0.20177282392978668, Exactitud: 92.34873962402344, Perdida de prueba: 0.47482597827911377, Exactitud de prueba: 86.27999877929688\n",
            "Epoch 56, Perdida: 0.2022230178117752, Exactitud: 92.51383972167969, Perdida de prueba: 0.4370133876800537, Exactitud de prueba: 87.30000305175781\n",
            "Epoch 57, Perdida: 0.19847120344638824, Exactitud: 92.63724517822266, Perdida de prueba: 0.48457109928131104, Exactitud de prueba: 87.04000091552734\n",
            "Epoch 58, Perdida: 0.19506648182868958, Exactitud: 92.71729278564453, Perdida de prueba: 0.45053642988204956, Exactitud de prueba: 87.37000274658203\n",
            "Epoch 59, Perdida: 0.19315263628959656, Exactitud: 92.72730255126953, Perdida de prueba: 0.47192612290382385, Exactitud de prueba: 86.97000122070312\n",
            "Epoch 60, Perdida: 0.19447404146194458, Exactitud: 92.66393280029297, Perdida de prueba: 0.46701034903526306, Exactitud de prueba: 86.88999938964844\n",
            "Epoch 61, Perdida: 0.1867111772298813, Exactitud: 93.0124740600586, Perdida de prueba: 0.46806055307388306, Exactitud de prueba: 87.54000091552734\n",
            "Epoch 62, Perdida: 0.1785035878419876, Exactitud: 93.20092010498047, Perdida de prueba: 0.4803960919380188, Exactitud de prueba: 87.83000183105469\n",
            "Epoch 63, Perdida: 0.1839120090007782, Exactitud: 93.15255737304688, Perdida de prueba: 0.541128933429718, Exactitud de prueba: 86.45999908447266\n",
            "Epoch 64, Perdida: 0.18160752952098846, Exactitud: 93.15589141845703, Perdida de prueba: 0.5035686492919922, Exactitud de prueba: 87.23999786376953\n",
            "Epoch 65, Perdida: 0.18704046308994293, Exactitud: 93.03582000732422, Perdida de prueba: 0.5087936520576477, Exactitud de prueba: 87.56999969482422\n",
            "Epoch 66, Perdida: 0.17371806502342224, Exactitud: 93.41104888916016, Perdida de prueba: 0.5105685591697693, Exactitud de prueba: 87.44000244140625\n",
            "Epoch 67, Perdida: 0.175248384475708, Exactitud: 93.47608947753906, Perdida de prueba: 0.46660950779914856, Exactitud de prueba: 87.8499984741211\n",
            "Epoch 68, Perdida: 0.17219693958759308, Exactitud: 93.55113220214844, Perdida de prueba: 0.5535694360733032, Exactitud de prueba: 86.3699951171875\n",
            "Epoch 69, Perdida: 0.16899919509887695, Exactitud: 93.6578598022461, Perdida de prueba: 0.5099385976791382, Exactitud de prueba: 87.76000213623047\n",
            "Epoch 70, Perdida: 0.1718219518661499, Exactitud: 93.6628646850586, Perdida de prueba: 0.5466260313987732, Exactitud de prueba: 87.36000061035156\n",
            "Epoch 71, Perdida: 0.17224997282028198, Exactitud: 93.62784576416016, Perdida de prueba: 0.5801371335983276, Exactitud de prueba: 86.77999877929688\n",
            "Epoch 72, Perdida: 0.17149534821510315, Exactitud: 93.68621063232422, Perdida de prueba: 0.5628491044044495, Exactitud de prueba: 87.08999633789062\n",
            "Epoch 73, Perdida: 0.168221578001976, Exactitud: 93.7662582397461, Perdida de prueba: 0.584007203578949, Exactitud de prueba: 86.76000213623047\n",
            "Epoch 74, Perdida: 0.17663952708244324, Exactitud: 93.5611343383789, Perdida de prueba: 0.5966492295265198, Exactitud de prueba: 86.33000183105469\n",
            "Epoch 75, Perdida: 0.17040662467479706, Exactitud: 93.7712631225586, Perdida de prueba: 0.5510817170143127, Exactitud de prueba: 87.44000244140625\n",
            "Epoch 76, Perdida: 0.17228984832763672, Exactitud: 93.62784576416016, Perdida de prueba: 0.5551149249076843, Exactitud de prueba: 86.86000061035156\n",
            "Epoch 77, Perdida: 0.16769073903560638, Exactitud: 93.77459716796875, Perdida de prueba: 0.5202991962432861, Exactitud de prueba: 87.44000244140625\n",
            "Epoch 78, Perdida: 0.16778415441513062, Exactitud: 93.82463073730469, Perdida de prueba: 0.5366756916046143, Exactitud de prueba: 87.44999694824219\n",
            "Epoch 79, Perdida: 0.16580502688884735, Exactitud: 93.84797668457031, Perdida de prueba: 0.5821540951728821, Exactitud de prueba: 87.5\n",
            "Epoch 80, Perdida: 0.16580849885940552, Exactitud: 93.94636535644531, Perdida de prueba: 0.5828567147254944, Exactitud de prueba: 87.66999816894531\n",
            "Epoch 81, Perdida: 0.16780713200569153, Exactitud: 93.96638488769531, Perdida de prueba: 0.5438302159309387, Exactitud de prueba: 87.3499984741211\n",
            "Epoch 82, Perdida: 0.1659143716096878, Exactitud: 93.95470428466797, Perdida de prueba: 0.6691973209381104, Exactitud de prueba: 86.33999633789062\n",
            "Epoch 83, Perdida: 0.17271924018859863, Exactitud: 93.66619873046875, Perdida de prueba: 0.5926781296730042, Exactitud de prueba: 87.01000213623047\n",
            "Epoch 84, Perdida: 0.1622169017791748, Exactitud: 94.06644439697266, Perdida de prueba: 0.6126516461372375, Exactitud de prueba: 87.31999969482422\n",
            "Epoch 85, Perdida: 0.1556830108165741, Exactitud: 94.24821472167969, Perdida de prueba: 0.6076119542121887, Exactitud de prueba: 87.01000213623047\n",
            "Epoch 86, Perdida: 0.15659870207309723, Exactitud: 94.30992126464844, Perdida de prueba: 0.6599056720733643, Exactitud de prueba: 87.29000091552734\n",
            "Epoch 87, Perdida: 0.15961581468582153, Exactitud: 94.21986389160156, Perdida de prueba: 0.5791499018669128, Exactitud de prueba: 87.75\n",
            "Epoch 88, Perdida: 0.1509043425321579, Exactitud: 94.33159637451172, Perdida de prueba: 0.5877223014831543, Exactitud de prueba: 87.4000015258789\n",
            "Epoch 89, Perdida: 0.1552830934524536, Exactitud: 94.38330078125, Perdida de prueba: 0.6070082187652588, Exactitud de prueba: 87.66999816894531\n",
            "Epoch 90, Perdida: 0.166122704744339, Exactitud: 94.12480926513672, Perdida de prueba: 0.5244697332382202, Exactitud de prueba: 87.87000274658203\n",
            "Epoch 91, Perdida: 0.15330101549625397, Exactitud: 94.43999481201172, Perdida de prueba: 0.5322601795196533, Exactitud de prueba: 88.5999984741211\n",
            "Epoch 92, Perdida: 0.14577125012874603, Exactitud: 94.65345764160156, Perdida de prueba: 0.5528249740600586, Exactitud de prueba: 88.11000061035156\n",
            "Epoch 93, Perdida: 0.14395345747470856, Exactitud: 94.65846252441406, Perdida de prueba: 0.5789990425109863, Exactitud de prueba: 88.11000061035156\n",
            "Epoch 94, Perdida: 0.14873501658439636, Exactitud: 94.59342193603516, Perdida de prueba: 0.6082713007926941, Exactitud de prueba: 88.2300033569336\n",
            "Epoch 95, Perdida: 0.1373976767063141, Exactitud: 94.87525939941406, Perdida de prueba: 0.5259246230125427, Exactitud de prueba: 88.34000396728516\n",
            "Epoch 96, Perdida: 0.12831555306911469, Exactitud: 95.29217529296875, Perdida de prueba: 0.5472494959831238, Exactitud de prueba: 88.02000427246094\n",
            "Epoch 97, Perdida: 0.12798242270946503, Exactitud: 95.24881744384766, Perdida de prueba: 0.5335993766784668, Exactitud de prueba: 88.18000030517578\n",
            "Epoch 98, Perdida: 0.13811279833316803, Exactitud: 94.92362213134766, Perdida de prueba: 0.5520328879356384, Exactitud de prueba: 88.12000274658203\n",
            "Epoch 99, Perdida: 0.13493986427783966, Exactitud: 94.95530700683594, Perdida de prueba: 0.5712669491767883, Exactitud de prueba: 87.80999755859375\n",
            "Epoch 100, Perdida: 0.13315735757350922, Exactitud: 95.14875793457031, Perdida de prueba: 0.5242016315460205, Exactitud de prueba: 88.16000366210938\n",
            "Epoch 101, Perdida: 0.13435406982898712, Exactitud: 95.0787124633789, Perdida de prueba: 0.5247386693954468, Exactitud de prueba: 88.0999984741211\n",
            "Epoch 102, Perdida: 0.13689342141151428, Exactitud: 95.04202270507812, Perdida de prueba: 0.5997921824455261, Exactitud de prueba: 87.30999755859375\n",
            "Epoch 103, Perdida: 0.13575293123722076, Exactitud: 95.10205841064453, Perdida de prueba: 0.554029643535614, Exactitud de prueba: 87.62999725341797\n",
            "Epoch 104, Perdida: 0.12649041414260864, Exactitud: 95.36888885498047, Perdida de prueba: 0.5645828247070312, Exactitud de prueba: 87.45999908447266\n",
            "Epoch 105, Perdida: 0.13291406631469727, Exactitud: 95.17710876464844, Perdida de prueba: 0.5781977772712708, Exactitud de prueba: 88.20999908447266\n",
            "Epoch 106, Perdida: 0.11522465944290161, Exactitud: 95.69074249267578, Perdida de prueba: 0.6160444617271423, Exactitud de prueba: 87.94000244140625\n",
            "Epoch 107, Perdida: 0.11353939771652222, Exactitud: 95.74911499023438, Perdida de prueba: 0.5944358706474304, Exactitud de prueba: 88.09000396728516\n",
            "Epoch 108, Perdida: 0.11600400507450104, Exactitud: 95.79747772216797, Perdida de prueba: 0.5721920132637024, Exactitud de prueba: 88.30000305175781\n",
            "Epoch 109, Perdida: 0.11875054240226746, Exactitud: 95.63237762451172, Perdida de prueba: 0.6220667958259583, Exactitud de prueba: 87.05999755859375\n",
            "Epoch 110, Perdida: 0.12112870812416077, Exactitud: 95.50897216796875, Perdida de prueba: 0.6179105639457703, Exactitud de prueba: 87.84000396728516\n",
            "Epoch 111, Perdida: 0.11932352185249329, Exactitud: 95.60236358642578, Perdida de prueba: 0.6404635906219482, Exactitud de prueba: 87.19999694824219\n",
            "Epoch 112, Perdida: 0.11356186121702194, Exactitud: 95.87419128417969, Perdida de prueba: 0.6435045003890991, Exactitud de prueba: 87.91999816894531\n",
            "Epoch 113, Perdida: 0.11718933284282684, Exactitud: 95.64905548095703, Perdida de prueba: 0.6660504341125488, Exactitud de prueba: 87.63999938964844\n",
            "Epoch 114, Perdida: 0.11262323707342148, Exactitud: 95.9292221069336, Perdida de prueba: 0.6481060981750488, Exactitud de prueba: 88.23999786376953\n",
            "Epoch 115, Perdida: 0.11021428555250168, Exactitud: 95.97425079345703, Perdida de prueba: 0.6419545412063599, Exactitud de prueba: 87.37999725341797\n",
            "Epoch 116, Perdida: 0.10935191065073013, Exactitud: 95.94090270996094, Perdida de prueba: 0.695969820022583, Exactitud de prueba: 87.47000122070312\n",
            "Epoch 117, Perdida: 0.10886473953723907, Exactitud: 95.9942626953125, Perdida de prueba: 0.674188494682312, Exactitud de prueba: 87.68000030517578\n",
            "Epoch 118, Perdida: 0.10939497500658035, Exactitud: 96.04429626464844, Perdida de prueba: 0.6794289350509644, Exactitud de prueba: 88.02000427246094\n",
            "Epoch 119, Perdida: 0.10975584387779236, Exactitud: 95.9392318725586, Perdida de prueba: 0.699467658996582, Exactitud de prueba: 88.25\n",
            "Epoch 120, Perdida: 0.11018294095993042, Exactitud: 95.97591400146484, Perdida de prueba: 0.6976650357246399, Exactitud de prueba: 87.95999908447266\n",
            "Epoch 121, Perdida: 0.10632216930389404, Exactitud: 96.08264923095703, Perdida de prueba: 0.6853646636009216, Exactitud de prueba: 87.94000244140625\n",
            "Epoch 122, Perdida: 0.10457183420658112, Exactitud: 96.23107147216797, Perdida de prueba: 0.7300372123718262, Exactitud de prueba: 88.27000427246094\n",
            "Epoch 123, Perdida: 0.10300593823194504, Exactitud: 96.20272064208984, Perdida de prueba: 0.719528079032898, Exactitud de prueba: 87.51000213623047\n",
            "Epoch 124, Perdida: 0.1128612533211708, Exactitud: 95.99259185791016, Perdida de prueba: 0.7319070100784302, Exactitud de prueba: 87.54000091552734\n",
            "Epoch 125, Perdida: 0.10673945397138596, Exactitud: 96.13935089111328, Perdida de prueba: 0.7151117920875549, Exactitud de prueba: 88.16999816894531\n",
            "Epoch 126, Perdida: 0.10615284740924835, Exactitud: 96.11433410644531, Perdida de prueba: 0.7413738965988159, Exactitud de prueba: 87.76000213623047\n",
            "Epoch 127, Perdida: 0.10112521052360535, Exactitud: 96.31612396240234, Perdida de prueba: 0.7030117511749268, Exactitud de prueba: 87.95999908447266\n",
            "Epoch 128, Perdida: 0.10005524754524231, Exactitud: 96.314453125, Perdida de prueba: 0.720742404460907, Exactitud de prueba: 88.44000244140625\n",
            "Epoch 129, Perdida: 0.09202980250120163, Exactitud: 96.59962463378906, Perdida de prueba: 0.7801012992858887, Exactitud de prueba: 88.16999816894531\n",
            "Epoch 130, Perdida: 0.10297305881977081, Exactitud: 96.30945587158203, Perdida de prueba: 0.6671238541603088, Exactitud de prueba: 88.20999908447266\n",
            "Epoch 131, Perdida: 0.09330054372549057, Exactitud: 96.51957702636719, Perdida de prueba: 0.6920372843742371, Exactitud de prueba: 88.05000305175781\n",
            "Epoch 132, Perdida: 0.09174156188964844, Exactitud: 96.5912857055664, Perdida de prueba: 0.8225347399711609, Exactitud de prueba: 88.02999877929688\n",
            "Epoch 133, Perdida: 0.10255344212055206, Exactitud: 96.29277801513672, Perdida de prueba: 0.7720698118209839, Exactitud de prueba: 88.0999984741211\n",
            "Epoch 134, Perdida: 0.09854498505592346, Exactitud: 96.30778503417969, Perdida de prueba: 0.7412745356559753, Exactitud de prueba: 88.16000366210938\n",
            "Epoch 135, Perdida: 0.0917140543460846, Exactitud: 96.66967010498047, Perdida de prueba: 0.8146218657493591, Exactitud de prueba: 88.05999755859375\n",
            "Epoch 136, Perdida: 0.08753444999456406, Exactitud: 96.7964096069336, Perdida de prueba: 0.7995972633361816, Exactitud de prueba: 87.41000366210938\n",
            "Epoch 137, Perdida: 0.09246381372213364, Exactitud: 96.68134307861328, Perdida de prueba: 0.7101370692253113, Exactitud de prueba: 88.41000366210938\n",
            "Epoch 138, Perdida: 0.08616802841424942, Exactitud: 96.82976531982422, Perdida de prueba: 0.747839093208313, Exactitud de prueba: 88.41000366210938\n",
            "Epoch 139, Perdida: 0.09142617136240005, Exactitud: 96.74471282958984, Perdida de prueba: 0.756438672542572, Exactitud de prueba: 88.01000213623047\n",
            "Epoch 140, Perdida: 0.09140609949827194, Exactitud: 96.6546630859375, Perdida de prueba: 0.8723816275596619, Exactitud de prueba: 88.23999786376953\n",
            "Epoch 141, Perdida: 0.0984131246805191, Exactitud: 96.5362548828125, Perdida de prueba: 0.7316980957984924, Exactitud de prueba: 88.3800048828125\n",
            "Epoch 142, Perdida: 0.08624892681837082, Exactitud: 96.79473876953125, Perdida de prueba: 0.8133349418640137, Exactitud de prueba: 88.31999969482422\n",
            "Epoch 143, Perdida: 0.09273931384086609, Exactitud: 96.748046875, Perdida de prueba: 0.7105976343154907, Exactitud de prueba: 87.86000061035156\n",
            "Epoch 144, Perdida: 0.10567419975996017, Exactitud: 96.14935302734375, Perdida de prueba: 0.7504013776779175, Exactitud de prueba: 88.30000305175781\n",
            "Epoch 145, Perdida: 0.09138314425945282, Exactitud: 96.74137878417969, Perdida de prueba: 0.7741314172744751, Exactitud de prueba: 88.22000122070312\n",
            "Epoch 146, Perdida: 0.08441738784313202, Exactitud: 96.95149993896484, Perdida de prueba: 0.7674285173416138, Exactitud de prueba: 88.2300033569336\n",
            "Epoch 147, Perdida: 0.08956950902938843, Exactitud: 96.78640747070312, Perdida de prueba: 0.7306397557258606, Exactitud de prueba: 88.47000122070312\n",
            "Epoch 148, Perdida: 0.086213618516922, Exactitud: 96.86811828613281, Perdida de prueba: 0.7457258105278015, Exactitud de prueba: 88.02999877929688\n",
            "Epoch 149, Perdida: 0.08069188892841339, Exactitud: 97.05823516845703, Perdida de prueba: 0.7790703773498535, Exactitud de prueba: 88.06999969482422\n",
            "Epoch 150, Perdida: 0.08275113999843597, Exactitud: 96.96150970458984, Perdida de prueba: 0.8360353708267212, Exactitud de prueba: 87.9000015258789\n",
            "Epoch 151, Perdida: 0.08491121232509613, Exactitud: 96.9314956665039, Perdida de prueba: 0.8127089142799377, Exactitud de prueba: 87.87999725341797\n",
            "Epoch 152, Perdida: 0.07845627516508102, Exactitud: 97.09825897216797, Perdida de prueba: 0.8171671628952026, Exactitud de prueba: 88.3499984741211\n",
            "Epoch 153, Perdida: 0.08555250614881516, Exactitud: 96.9314956665039, Perdida de prueba: 0.8516390919685364, Exactitud de prueba: 88.02999877929688\n",
            "Epoch 154, Perdida: 0.0930015817284584, Exactitud: 96.67300415039062, Perdida de prueba: 0.7382633090019226, Exactitud de prueba: 88.11000061035156\n",
            "Epoch 155, Perdida: 0.07505615800619125, Exactitud: 97.22000122070312, Perdida de prueba: 0.8076984286308289, Exactitud de prueba: 88.58000183105469\n",
            "Epoch 156, Perdida: 0.07999388873577118, Exactitud: 97.09659576416016, Perdida de prueba: 0.9117162227630615, Exactitud de prueba: 87.62000274658203\n",
            "Epoch 157, Perdida: 0.09288828074932098, Exactitud: 96.86811828613281, Perdida de prueba: 0.843148410320282, Exactitud de prueba: 88.2300033569336\n",
            "Epoch 158, Perdida: 0.09108047932386398, Exactitud: 96.87145233154297, Perdida de prueba: 0.7154542207717896, Exactitud de prueba: 88.30000305175781\n",
            "Epoch 159, Perdida: 0.08390916883945465, Exactitud: 97.08491516113281, Perdida de prueba: 0.8693052530288696, Exactitud de prueba: 87.91999816894531\n",
            "Epoch 160, Perdida: 0.07979896664619446, Exactitud: 97.09659576416016, Perdida de prueba: 0.8150086998939514, Exactitud de prueba: 88.34000396728516\n",
            "Epoch 161, Perdida: 0.0682750716805458, Exactitud: 97.4584732055664, Perdida de prueba: 0.8159369826316833, Exactitud de prueba: 88.08000183105469\n",
            "Epoch 162, Perdida: 0.0747339129447937, Exactitud: 97.38509368896484, Perdida de prueba: 0.7948727607727051, Exactitud de prueba: 87.98999786376953\n",
            "Epoch 163, Perdida: 0.07782948017120361, Exactitud: 97.18331146240234, Perdida de prueba: 0.8295932412147522, Exactitud de prueba: 87.93000030517578\n",
            "Epoch 164, Perdida: 0.07324668020009995, Exactitud: 97.44847106933594, Perdida de prueba: 0.8378908038139343, Exactitud de prueba: 88.30999755859375\n",
            "Epoch 165, Perdida: 0.0783044621348381, Exactitud: 97.16163635253906, Perdida de prueba: 0.8892927765846252, Exactitud de prueba: 87.97000122070312\n",
            "Epoch 166, Perdida: 0.07408681511878967, Exactitud: 97.21332550048828, Perdida de prueba: 0.9759654402732849, Exactitud de prueba: 88.47000122070312\n",
            "Epoch 167, Perdida: 0.07154393196105957, Exactitud: 97.47682189941406, Perdida de prueba: 0.8307724595069885, Exactitud de prueba: 88.34000396728516\n",
            "Epoch 168, Perdida: 0.07238909602165222, Exactitud: 97.31005096435547, Perdida de prueba: 0.8728833198547363, Exactitud de prueba: 88.4000015258789\n",
            "Epoch 169, Perdida: 0.07720903307199478, Exactitud: 97.20332336425781, Perdida de prueba: 0.8018679618835449, Exactitud de prueba: 88.25\n",
            "Epoch 170, Perdida: 0.06924375146627426, Exactitud: 97.47848510742188, Perdida de prueba: 0.8992740511894226, Exactitud de prueba: 88.13999938964844\n",
            "Epoch 171, Perdida: 0.07852678745985031, Exactitud: 97.18997955322266, Perdida de prueba: 0.8239807486534119, Exactitud de prueba: 88.16999816894531\n",
            "Epoch 172, Perdida: 0.07647895812988281, Exactitud: 97.28504180908203, Perdida de prueba: 0.8106476664543152, Exactitud de prueba: 87.83000183105469\n",
            "Epoch 173, Perdida: 0.07509581744670868, Exactitud: 97.33006286621094, Perdida de prueba: 0.8843609094619751, Exactitud de prueba: 88.33000183105469\n",
            "Epoch 174, Perdida: 0.07900332659482956, Exactitud: 97.30504608154297, Perdida de prueba: 0.8283470869064331, Exactitud de prueba: 88.13999938964844\n",
            "Epoch 175, Perdida: 0.08333881944417953, Exactitud: 97.09325408935547, Perdida de prueba: 0.8645739555358887, Exactitud de prueba: 88.05000305175781\n",
            "Epoch 176, Perdida: 0.07098507136106491, Exactitud: 97.45013427734375, Perdida de prueba: 0.8783882856369019, Exactitud de prueba: 88.48999786376953\n",
            "Epoch 177, Perdida: 0.06910592317581177, Exactitud: 97.51850891113281, Perdida de prueba: 0.919924795627594, Exactitud de prueba: 88.05000305175781\n",
            "Epoch 178, Perdida: 0.07412352412939072, Exactitud: 97.49015808105469, Perdida de prueba: 0.9295726418495178, Exactitud de prueba: 88.26000213623047\n",
            "Epoch 179, Perdida: 0.06079454720020294, Exactitud: 97.83203125, Perdida de prueba: 0.9188284277915955, Exactitud de prueba: 88.36000061035156\n",
            "Epoch 180, Perdida: 0.0644199326634407, Exactitud: 97.60856628417969, Perdida de prueba: 0.9266689419746399, Exactitud de prueba: 88.1500015258789\n",
            "Epoch 181, Perdida: 0.07708031684160233, Exactitud: 97.33340454101562, Perdida de prueba: 0.8118844032287598, Exactitud de prueba: 87.97000122070312\n",
            "Epoch 182, Perdida: 0.06621109694242477, Exactitud: 97.58021545410156, Perdida de prueba: 0.9810870885848999, Exactitud de prueba: 88.02999877929688\n",
            "Epoch 183, Perdida: 0.06298734247684479, Exactitud: 97.74198150634766, Perdida de prueba: 0.8587302565574646, Exactitud de prueba: 88.16000366210938\n",
            "Epoch 184, Perdida: 0.0677260309457779, Exactitud: 97.6736068725586, Perdida de prueba: 0.9480162858963013, Exactitud de prueba: 88.01000213623047\n",
            "Epoch 185, Perdida: 0.08125591278076172, Exactitud: 97.2566909790039, Perdida de prueba: 0.804763913154602, Exactitud de prueba: 88.0999984741211\n",
            "Epoch 186, Perdida: 0.07070774585008621, Exactitud: 97.57353973388672, Perdida de prueba: 1.0800317525863647, Exactitud de prueba: 87.15999603271484\n",
            "Epoch 187, Perdida: 0.06728802621364594, Exactitud: 97.65192413330078, Perdida de prueba: 0.9172093868255615, Exactitud de prueba: 87.9800033569336\n",
            "Epoch 188, Perdida: 0.06170142441987991, Exactitud: 97.7986831665039, Perdida de prueba: 0.9293047785758972, Exactitud de prueba: 88.59000396728516\n",
            "Epoch 189, Perdida: 0.06012791767716408, Exactitud: 97.86538696289062, Perdida de prueba: 0.996661901473999, Exactitud de prueba: 88.02999877929688\n",
            "Epoch 190, Perdida: 0.07559789717197418, Exactitud: 97.44847106933594, Perdida de prueba: 0.9261853694915771, Exactitud de prueba: 88.52999877929688\n",
            "Epoch 191, Perdida: 0.06733772903680801, Exactitud: 97.65026092529297, Perdida de prueba: 0.8743010759353638, Exactitud de prueba: 88.66000366210938\n",
            "Epoch 192, Perdida: 0.05816435441374779, Exactitud: 97.88873291015625, Perdida de prueba: 0.9371399283409119, Exactitud de prueba: 88.56999969482422\n",
            "Epoch 193, Perdida: 0.057812754064798355, Exactitud: 97.87039184570312, Perdida de prueba: 0.9314736127853394, Exactitud de prueba: 87.93000030517578\n",
            "Epoch 194, Perdida: 0.06351178884506226, Exactitud: 97.74864959716797, Perdida de prueba: 0.951894223690033, Exactitud de prueba: 87.9800033569336\n",
            "Epoch 195, Perdida: 0.0659622773528099, Exactitud: 97.63524627685547, Perdida de prueba: 0.9455410242080688, Exactitud de prueba: 88.01000213623047\n",
            "Epoch 196, Perdida: 0.07056792825460434, Exactitud: 97.54352569580078, Perdida de prueba: 0.9767928719520569, Exactitud de prueba: 88.27999877929688\n",
            "Epoch 197, Perdida: 0.06475166231393814, Exactitud: 97.77366638183594, Perdida de prueba: 0.9355641603469849, Exactitud de prueba: 87.98999786376953\n",
            "Epoch 198, Perdida: 0.06183173879981041, Exactitud: 97.87372589111328, Perdida de prueba: 0.9217339158058167, Exactitud de prueba: 88.01000213623047\n",
            "Epoch 199, Perdida: 0.059637539088726044, Exactitud: 97.90541076660156, Perdida de prueba: 0.9634382128715515, Exactitud de prueba: 88.19000244140625\n",
            "Epoch 200, Perdida: 0.07430995255708694, Exactitud: 97.44513702392578, Perdida de prueba: 0.9549840092658997, Exactitud de prueba: 87.58000183105469\n",
            "Epoch 201, Perdida: 0.06437142938375473, Exactitud: 97.70529174804688, Perdida de prueba: 1.0194672346115112, Exactitud de prueba: 88.22000122070312\n",
            "Epoch 202, Perdida: 0.0586712509393692, Exactitud: 97.84370422363281, Perdida de prueba: 0.9838851690292358, Exactitud de prueba: 88.23999786376953\n",
            "Epoch 203, Perdida: 0.054258041083812714, Exactitud: 98.06217193603516, Perdida de prueba: 1.0171295404434204, Exactitud de prueba: 88.0999984741211\n",
            "Epoch 204, Perdida: 0.05117981880903244, Exactitud: 98.2105941772461, Perdida de prueba: 1.0104990005493164, Exactitud de prueba: 88.37000274658203\n",
            "Epoch 205, Perdida: 0.06007295474410057, Exactitud: 97.85037994384766, Perdida de prueba: 0.9801554679870605, Exactitud de prueba: 88.18000030517578\n",
            "Epoch 206, Perdida: 0.06994614005088806, Exactitud: 97.55353546142578, Perdida de prueba: 0.9853922724723816, Exactitud de prueba: 88.33000183105469\n",
            "Epoch 207, Perdida: 0.06970146298408508, Exactitud: 97.61356353759766, Perdida de prueba: 0.8906922936439514, Exactitud de prueba: 88.02999877929688\n",
            "Epoch 208, Perdida: 0.06102925166487694, Exactitud: 97.91874694824219, Perdida de prueba: 0.9943786263465881, Exactitud de prueba: 88.5\n",
            "Epoch 209, Perdida: 0.0602290965616703, Exactitud: 97.9404296875, Perdida de prueba: 0.9318897724151611, Exactitud de prueba: 88.43000030517578\n",
            "Epoch 210, Perdida: 0.04827282950282097, Exactitud: 98.2706298828125, Perdida de prueba: 1.0458534955978394, Exactitud de prueba: 88.59000396728516\n",
            "Epoch 211, Perdida: 0.05613265186548233, Exactitud: 98.03048706054688, Perdida de prueba: 0.9668304920196533, Exactitud de prueba: 88.51000213623047\n",
            "Epoch 212, Perdida: 0.0540466271340847, Exactitud: 98.07550811767578, Perdida de prueba: 1.0559437274932861, Exactitud de prueba: 88.23999786376953\n",
            "Epoch 213, Perdida: 0.05847092345356941, Exactitud: 97.96878051757812, Perdida de prueba: 0.9423433542251587, Exactitud de prueba: 87.95999908447266\n",
            "Epoch 214, Perdida: 0.06973190605640411, Exactitud: 97.64358520507812, Perdida de prueba: 1.0344651937484741, Exactitud de prueba: 87.70999908447266\n",
            "Epoch 215, Perdida: 0.07235249876976013, Exactitud: 97.60856628417969, Perdida de prueba: 0.9819899201393127, Exactitud de prueba: 88.16000366210938\n",
            "Epoch 216, Perdida: 0.049883000552654266, Exactitud: 98.2172622680664, Perdida de prueba: 1.0996595621109009, Exactitud de prueba: 87.80000305175781\n",
            "Epoch 217, Perdida: 0.05665719136595726, Exactitud: 98.06884002685547, Perdida de prueba: 1.042144775390625, Exactitud de prueba: 87.86000061035156\n",
            "Epoch 218, Perdida: 0.05541955679655075, Exactitud: 98.03715515136719, Perdida de prueba: 1.0778708457946777, Exactitud de prueba: 87.7300033569336\n",
            "Epoch 219, Perdida: 0.06000359728932381, Exactitud: 98.0004653930664, Perdida de prueba: 1.056027889251709, Exactitud de prueba: 87.37999725341797\n",
            "Epoch 220, Perdida: 0.05482563003897667, Exactitud: 98.13388061523438, Perdida de prueba: 1.0327774286270142, Exactitud de prueba: 87.9800033569336\n",
            "Epoch 221, Perdida: 0.0560312382876873, Exactitud: 98.09886169433594, Perdida de prueba: 1.060325026512146, Exactitud de prueba: 88.11000061035156\n",
            "Epoch 222, Perdida: 0.055228091776371, Exactitud: 98.09886169433594, Perdida de prueba: 0.9910988211631775, Exactitud de prueba: 88.11000061035156\n",
            "Epoch 223, Perdida: 0.05900038033723831, Exactitud: 98.01547241210938, Perdida de prueba: 1.0340571403503418, Exactitud de prueba: 88.12000274658203\n",
            "Epoch 224, Perdida: 0.051244549453258514, Exactitud: 98.2122573852539, Perdida de prueba: 1.1044812202453613, Exactitud de prueba: 87.75\n",
            "Epoch 225, Perdida: 0.06447505950927734, Exactitud: 97.93876647949219, Perdida de prueba: 0.9626185297966003, Exactitud de prueba: 88.06999969482422\n",
            "Epoch 226, Perdida: 0.057369910180568695, Exactitud: 98.11720275878906, Perdida de prueba: 1.049735188484192, Exactitud de prueba: 88.09000396728516\n",
            "Epoch 227, Perdida: 0.052874933928251266, Exactitud: 98.15555572509766, Perdida de prueba: 0.9832710027694702, Exactitud de prueba: 88.13999938964844\n",
            "Epoch 228, Perdida: 0.05273598060011864, Exactitud: 98.1088638305664, Perdida de prueba: 0.9671727418899536, Exactitud de prueba: 88.27999877929688\n",
            "Epoch 229, Perdida: 0.059738580137491226, Exactitud: 98.03549194335938, Perdida de prueba: 0.9715134501457214, Exactitud de prueba: 87.5999984741211\n",
            "Epoch 230, Perdida: 0.053836699575185776, Exactitud: 98.09886169433594, Perdida de prueba: 1.0183641910552979, Exactitud de prueba: 87.8499984741211\n",
            "Epoch 231, Perdida: 0.04554038494825363, Exactitud: 98.44906616210938, Perdida de prueba: 1.0364782810211182, Exactitud de prueba: 88.16000366210938\n",
            "Epoch 232, Perdida: 0.05508485063910484, Exactitud: 98.12721252441406, Perdida de prueba: 0.998864471912384, Exactitud de prueba: 87.94000244140625\n",
            "Epoch 233, Perdida: 0.0595163069665432, Exactitud: 98.10386657714844, Perdida de prueba: 1.0035138130187988, Exactitud de prueba: 87.25\n",
            "Epoch 234, Perdida: 0.0612977035343647, Exactitud: 97.96878051757812, Perdida de prueba: 0.9575641751289368, Exactitud de prueba: 87.75\n",
            "Epoch 235, Perdida: 0.04388333857059479, Exactitud: 98.37568664550781, Perdida de prueba: 1.0524605512619019, Exactitud de prueba: 88.36000061035156\n",
            "Epoch 236, Perdida: 0.05729284510016441, Exactitud: 98.15055847167969, Perdida de prueba: 0.9695703387260437, Exactitud de prueba: 88.04000091552734\n",
            "Epoch 237, Perdida: 0.04290482774376869, Exactitud: 98.51744079589844, Perdida de prueba: 1.1473841667175293, Exactitud de prueba: 87.5\n",
            "Epoch 238, Perdida: 0.05805657431483269, Exactitud: 98.0054702758789, Perdida de prueba: 1.0092931985855103, Exactitud de prueba: 88.3800048828125\n",
            "Epoch 239, Perdida: 0.04800919070839882, Exactitud: 98.38736724853516, Perdida de prueba: 1.0467023849487305, Exactitud de prueba: 88.30000305175781\n",
            "Epoch 240, Perdida: 0.050616372376680374, Exactitud: 98.32732391357422, Perdida de prueba: 1.0551307201385498, Exactitud de prueba: 87.58999633789062\n",
            "Epoch 241, Perdida: 0.05931772291660309, Exactitud: 98.01380920410156, Perdida de prueba: 0.9905548095703125, Exactitud de prueba: 87.91999816894531\n",
            "Epoch 242, Perdida: 0.05041959509253502, Exactitud: 98.31231689453125, Perdida de prueba: 1.0631532669067383, Exactitud de prueba: 87.86000061035156\n",
            "Epoch 243, Perdida: 0.05106965824961662, Exactitud: 98.34734344482422, Perdida de prueba: 1.0416536331176758, Exactitud de prueba: 88.11000061035156\n",
            "Epoch 244, Perdida: 0.05068570002913475, Exactitud: 98.25895690917969, Perdida de prueba: 1.043561577796936, Exactitud de prueba: 88.06999969482422\n",
            "Epoch 245, Perdida: 0.054584745317697525, Exactitud: 98.12053680419922, Perdida de prueba: 1.0394030809402466, Exactitud de prueba: 87.98999786376953\n",
            "Epoch 246, Perdida: 0.05391036346554756, Exactitud: 98.22059631347656, Perdida de prueba: 1.0481624603271484, Exactitud de prueba: 87.63999938964844\n",
            "Epoch 247, Perdida: 0.06231226772069931, Exactitud: 98.06717681884766, Perdida de prueba: 0.946901261806488, Exactitud de prueba: 87.94999694824219\n",
            "Epoch 248, Perdida: 0.05038943514227867, Exactitud: 98.31565856933594, Perdida de prueba: 1.0170097351074219, Exactitud de prueba: 88.68000030517578\n",
            "Epoch 249, Perdida: 0.040167517960071564, Exactitud: 98.61083221435547, Perdida de prueba: 1.0890698432922363, Exactitud de prueba: 88.05000305175781\n",
            "Epoch 250, Perdida: 0.04534018039703369, Exactitud: 98.45407104492188, Perdida de prueba: 1.10123872756958, Exactitud de prueba: 88.34000396728516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones\n",
        "\n",
        "**A pesar de haber obtenido una exactitud en pruebas de 88.340%, no podemos asegurar que no exisita overfiting, es decir, un sobreentrenamiento. Donde nuestro modelo en lugar de generalizar, particularizó el conjunto de entrenamiento. Otro dato importante es que el uso del regularizador para iniciar los pesos en valores más pequeños ayudó a mejorar el entrenamiento, pues sin él, existían mesetas donde la exactitud alcanzaba un 84% pero en épocas posteriores bajaba hasta un 60%**"
      ],
      "metadata": {
        "id": "EziecHDzE9t4"
      }
    }
  ]
}